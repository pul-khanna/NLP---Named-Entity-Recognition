{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><b> Named Entity Recognition </b></center></h1>\n",
    "\n",
    "#### NER is used for extraction such entities from the text as persons, organizations, locations, etc. Here we try to recognize named entities from Twitter with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will use a recurrent neural network model which will be made using KERAS Functional API and to solve our NER problem. In addition we will use Pre-Trained GLoVe Embedding, Many to Many Architecture for last layer(TimeDistributed) and for monitoring and evaluation we will use Callbacks such EarlyStopping and TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide tensorflow warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Libraries needed\n",
    "\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model, optimizers\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Activation,\n",
    "    Bidirectional,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    TimeDistributed,\n",
    ")\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Sometimes we have to run the above cell for importing libraries again as tensorflow or keras module is unable to get imported the first time due to some bug. If later on, if we get an error when compiling our mode or our settings optimizers etc, re-run the import library cell again.\n",
    "#### Note: Tensorflow 2.5 and numpy 1.20 and 1.21 will result in an error when building the model as Tensforflow uses numpy ver 1.19.5 internally for calculations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT O\\n', '@TheValarium O\\n', ': O\\n', 'Online O\\n', 'ticket O\\n', 'sales O\\n', 'for O\\n', 'Ghostland B-musicartist\\n', 'Observatory I-musicartist\\n', 'extended O\\n', 'until O\\n', '6 O\\n', 'PM O\\n', 'EST O\\n', 'due O\\n', 'to O\\n', 'high O\\n', 'demand O\\n', '. O\\n', 'Get O\\n', 'them O\\n', 'before O\\n', 'they O\\n', 'sell O\\n', 'out O\\n', '... O\\n', '\\n', 'Apple B-product\\n', 'MacBook I-product\\n', 'Pro I-product\\n', 'A1278 I-product\\n', '13.3 I-product\\n', '\" I-product\\n', 'Laptop I-product\\n', '- I-product\\n', 'MD101LL/A I-product\\n', '( O\\n', 'June O\\n', ', O\\n', '2012 O\\n', ') O\\n', '- O\\n', 'Full O\\n', 'read O\\n', 'by O\\n', 'eBay B-company\\n', 'http://t.co/2zgQ99nmuf O\\n', 'http://t.co/eQmogqqABK O\\n', '\\n', 'Happy O\\n', 'Birthday O\\n', '@AshForeverAshey O\\n', '! O\\n', 'May O\\n', 'Allah B-person\\n', 's.w.t O\\n', 'bless O\\n', 'you O\\n', 'with O\\n', 'goodness O\\n', 'and O\\n', 'happiness O\\n', '. O\\n', '\\n', '@AqwSkills O\\n', 'the O\\n', 'quest O\\n', 'line O\\n', 'im O\\n', 'assuming O\\n', 'it O\\n', 'will O\\n', 'be O\\n', 'the O\\n', 'same O\\n', 'way O\\n', 'with O\\n', 'awe O\\n', 'thur O\\n', '\\n', '@Wolven O\\n', '@VanessaY O\\n', '@miniver O\\n', '@mathpunk O\\n', '@NeilHarbisson O\\n', 'still O\\n', 'perception O\\n', ', O\\n', 'and O\\n', 'what O\\n', 'you O\\n', 'key O\\n', 'off O\\n', 'may O\\n', 'differ O\\n', 'from O\\n', 'me O\\n', '. O\\n', 'Back O\\n', 'to O\\n']\n"
     ]
    }
   ],
   "source": [
    "# Note: 'with' automatically closes file \n",
    "\n",
    "with open('data/train.txt') as f:\n",
    "    line = [f.readline() for i in range(100)]\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading twitter text file format, we note every line contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace and different tweets are separated by an empty line. \n",
    "\n",
    "#### Further, we see different user names start after an '@' symbol and also there are different url's. These can be replaced by an identification token as they won't be useful by themselves in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to read twitter text file and return a separated list of tokens and its corresponding tags \"\"\"\n",
    "\n",
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            if token.startswith(\"@\"):\n",
    "                token=\"<USR>\"\n",
    "            elif token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
    "                token=\"<URL>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "val_tokens, val_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving our tokens and tags into train,validation and test sets.\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Sentence 1 :- -----\n",
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "----------------------- \n",
      "\n",
      "----- Sentence 2 :- -----\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "----------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"----- Sentence {0} :- -----\".format(i+1))\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print (\"----------------------- \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking our training set list for tokens and tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To train our neural network, we will create a dictionary with two mappings:\n",
    "- token → token id: use integers as input data of KERAS embedding layer so that words can be represented     by dense vectors (More memory and computationally efficient than One-Hot Encoding)\n",
    "- tag → tag id: use integers for computing the loss at the output of the network.\n",
    "\n",
    "#### Note: Here we are considering that we have single-class labels, where an object can only belong to one class, i.e. each token belongs to a particular tag or vice-versa. Hence, we can use \"sparse_categorical_crossentropy\" instead of \"categorical_crossentropy\" as our loss function which requires one-hot-encoded vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: We will also need to add some 'Special tokens and tags' to our dictionary. They will be unknown tokens and padding tokens/tags.\n",
    "- &lt;UNK&gt; token to represent of vocabulary tokens;\n",
    "- &lt;PAD&gt; tokens and tags for padding sentence to the same length when we create batches of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dictionary to create mappings for tokens and tags \"\"\"\n",
    "\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "# Note: First add special tokens and tags to the dictionaries and the first special token must have index 0\n",
    "# Here, first special token must be for \"pad\" as later on we can use an in-built KERAS function for masking\n",
    "# pads to avoid extra copmutations i.e. mask_zero which only applies for value 0 (index 0 here)\n",
    "    \n",
    "    k = 0\n",
    "    for line in special_tokens:\n",
    "        tok2idx[line] = k\n",
    "        k += 1\n",
    "        idx2tok.append(line)\n",
    "        \n",
    "    for tokens in tokens_or_tags:\n",
    "        for token in tokens:\n",
    "            if token not in tok2idx: \n",
    "                tok2idx[token] = k \n",
    "                k += 1\n",
    "                idx2tok.append(token)\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<PAD>', '<UNK>']\n",
    "special_tags = ['<PAD>']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + val_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags + val_tags , special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Implemented the function build_dict and creating dictionaries for our tokens and tags.\n",
    "#### Note: We always consider test tokens and tags to be unseen/unknown and hence don't use them when creating our dicitonaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Input Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The LSTM layers only accepts sequences within a batch that are of same lengths. It means that weight updates of the network are based on several sequences at every single time. So we will pad shorter sequences within a batch with a special token (&lt;PAD&gt;) to match the length of the largest sequence in that batch, thus making all sequences of equal length for every batch. This technique is also known as bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will define a function to generate batches of equal length which can then be fed into our neural network model. For this, we will define 3 functions :-\n",
    "* helper function to generate the number of batches for a given batch size\n",
    "* helper function to create list of the mapped id's from tokens when given a sentence\n",
    "* function for generating batches with sequences of equal lengths by applying padding using the 2 helper functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_batches(batch_size, tokens, allow_smaller_last_batch = True):\n",
    "    n_samples = len(tokens)\n",
    "    n_batches = n_samples // batch_size                          ### '//' for floor division \n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "    return n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def tags2idxs(tags_list):\n",
    "    return [tag2idx[tag] for tag in tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, n_batches, tokens, tags,\n",
    "                      shuffle = True):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    while True:                       ### 'while' loop used for looping through epochs in train.fit() \n",
    "        n_samples = len(tokens)\n",
    "        if shuffle:\n",
    "            order = np.random.permutation(n_samples)\n",
    "        else:\n",
    "            order = np.arange(n_samples)\n",
    "    \n",
    "        for k in range(n_batches):\n",
    "            batch_start = k * batch_size\n",
    "            batch_end = min((k + 1) * batch_size, n_samples)\n",
    "            current_batch_size = batch_end - batch_start\n",
    "            x_list = []\n",
    "            y_list = []\n",
    "            max_len_token = 0\n",
    "            for idx in order[batch_start: batch_end]:\n",
    "                x_list.append(words2idxs(tokens[idx]))\n",
    "                y_list.append(tags2idxs(tags[idx]))\n",
    "                max_len_token = max(max_len_token, len(tags[idx]))\n",
    "                \n",
    "            # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "            x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "            y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['<PAD>']        \n",
    "            for n in range(current_batch_size):\n",
    "                utt_len = len(x_list[n])\n",
    "                x[n, :utt_len] = x_list[n]\n",
    "                y[n, :utt_len] = y_list[n]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: We will provide RNN and loss function with sequence lengths, so it can skip computations for padding parts. For this we can use KERAS mask_zero param to remove the paddings parts for computation.\n",
    "#### Helper function 'n_batches' has to be separate from 'batches_generator' function as later on we will be using 'fit' method for training our model which requires number of batches as input for steps_per_epoch param when using a generator as input for X(input data) param. However, when using 'train_on_batch' method instead of 'fit' method we can include them in a single function as well. For a similar reason we have used a 'while' loop in 'batches_generator' function to loop through epochs which will be declared in our 'fit' method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implementing Model Architecture\n",
    "\n",
    "#### We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification. We will use Keras Functional API to build our model.\n",
    "\n",
    "#### We will set our input shape as 'None' so that we can accept sequences of variable length\n",
    "\n",
    "#### 'mask_zero' param will be set to True in 'Embedding' layer so as to to avoid computations for padding tokens inside the RNN and also when computing loss function.\n",
    "\n",
    "#### We will set 'dropout' param in our RNN layer for regularization.\n",
    "\n",
    "#### Here we use 'Time Distributed Layer'(Many to Many architecture) instead of the single 'Dense Layer'(Many to One architecture) as output layer. The TimeDistributed applyies the same Dense layer (same weights) to the LSTMs outputs for one time step at a time. In this way, the output layer only needs one connection to each LSTM unit (plus one bias). For this reason, the number of training epochs needs to be increased to account for the smaller network capacity\n",
    "\n",
    "#### We will use 'softmax' in our activation function for output lyer, as it will provide us a multinominal probability distribution for our multi-label classification problem\n",
    "\n",
    "#### Our Loss Function will be \"sparse_categorical_crossentropy\" (No need for OHC, can use integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "- input_dim: This is the size of the vocabulary in the text data. Here, it will length of token dictionary   consisting of \n",
    "- output_dim: This is the size of the vector space in which words will be embedded. It defines the size of   the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger.       Here, we will use 200.\n",
    "- input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. Here, it will max sentence length from our training and validations dataset.\n",
    "\n",
    "#### Here we will use pre-trained GloVe (Global Vectors for Word Representation) Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVE Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the following command to download pre-trained GloVe embeddings (a 822M zip file):-\n",
    "* !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "* !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The archive contains text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones, i.e. \"glove.6B.100d.txt“.  In the file we can see that the token (word) is followed by the weights (100 numbers) on each line.\n",
    "#### To use pre-trained GloVe embeddings we will have to perform the following steps:-\n",
    "* load the whole embedding into memory\n",
    "* prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. It's a simple NumPy matrix where entry at index 'i' is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n",
    "* load the pre-trained word embeddings matrix into an Embedding layer.\n",
    "\n",
    "#### Note: Instead of loading the whole embedding into memory (speed up our process and save disk space), we can also filter the embeddings for the unique words in our training and validation data. To access the mapping of word to vector dictionary for our unique words we can use model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding layer must be defined with output_dim set to 100, as we chose the 100-dimensional text-encoded vectors. Also, Embedding layer is seeded with the GloVe word embedding weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(idx2token)\n",
    "output_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((input_dim, output_dim))\n",
    "for word, i in token2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = Embedding(input_dim = input_dim, output_dim = output_dim, \n",
    "                   trainable = False, mask_zero = True, \n",
    "                   embeddings_initializer = keras.initializers.Constant(embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: that we set trainable = False so as to keep the embeddings fixed (we don't want to update them (learned word weights) during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output dense layer length should be equal to number of labels i.e. tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = len(idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialized input and output parameters for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Optimizer for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use adam optimization algorithm with learning rate decay. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. An initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.\n",
    "#### Note: We can also use 'Adaptive Moment Estimation' instead of learning rate schedules method as it can be challenging to configure and is critical to the performance of a deep learning neural network model. \n",
    "\n",
    "#### We also apply clipping to eliminate exploding gradients. Here, we use 'global_clipnorm' function in KERAS which clips only gradients and not variables. Also, function 'clip_norm' clips the gradient of each weight independently of the gradients of the other weights and thus has the disadvantage of changing the descent direction, whereas with global_clipnorm the direction would remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = 1e-2,\n",
    "    decay_steps = 10000,\n",
    "    decay_rate = 0.9)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate = lr_schedule, global_clipnorm = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use EarlyStopping and Tensorboard callbacks. \n",
    "\n",
    "#### EarlyStopping is used so that we don’t need to hard code the number of epochs. If our network doesn’t improve for 2 consecutive epochs,i.e. validation loss is not decreased we are going to stop our training process. The 'patience' param will define that and as we are monitoring loss, our 'mode' param will be min and also we will set 'restore_best_weights' param as True.\n",
    "\n",
    "#### We will also use Tensorboard to monitor and evaluate our data which has a lot of different options and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1,                                                 write_graph = True, write_images = True, \n",
    "                                                update_freq = 'epoch', profile_batch = 2,\n",
    "                                                embeddings_freq = 1)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor ='val_loss', min_delta = 0, patience = 2, \n",
    "                                                  verbose = 0, mode = 'min', baseline = None, \n",
    "                                                  restore_best_weights = True)\n",
    "\n",
    "callbacks = [tensorboard_cb, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS BI-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         2050500   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 200)         160800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 22)          4422      \n",
      "=================================================================\n",
      "Total params: 2,215,722\n",
      "Trainable params: 165,222\n",
      "Non-trainable params: 2,050,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_seq = Input(shape = (None,))\n",
    "model = (embed_layer)(input_seq)\n",
    "\n",
    "forward_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(100, dropout = 0.5), return_sequences = True)\n",
    "backward_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(100, dropout = 0.5), return_sequences = True, go_backwards = True)\n",
    "\n",
    "model = (Bidirectional(forward_layer, backward_layer = backward_layer))(model)\n",
    "out = (TimeDistributed(Dense(num_tags, activation = \"softmax\")))(model)\n",
    "\n",
    "model = Model(input_seq, out)\n",
    "\n",
    "model.compile(optimizer = opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"Accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After compiling, we will train our data and conduct more experiments as needed to tune our hyperparameters, to obtain higher accuracy on the held-out validation dataset.\n",
    "#### The hyperparameters for our model are :- batch_size, epochs, learning_rate params, global_clip_norm, dropout.\n",
    "#### For training our model we will be using 'fit' method but in 'x' param (input data) and 'validation_data' param we will use a generator as sequences are of different lengths. Also, as we will be using a generator we will have assign values for 'steps_per_epoch' and 'validation_steps' pararms instead of batch_size.\n",
    "#### To use a generator for training our model we will define a function which will make use of 'n_batches' and 'batches_generator' functions defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" To train a model using a Generator \"\"\"\n",
    "\n",
    "def train_model(x_train , y_train, x_val = None, y_val = None, batch_size = 32, epochs = 2, verbose = 1):    \n",
    "    n_batch_train = n_batches(batch_size, x_train)    \n",
    "    if x_val:\n",
    "        n_batch_val = n_batches(batch_size, x_val)\n",
    "\n",
    "    history = model.fit(\n",
    "        x = batches_generator(batch_size, n_batch_train, x_train, y_train, epochs),\n",
    "        y = None,\n",
    "        steps_per_epoch = n_batch_train,\n",
    "        validation_data = batches_generator(batch_size, n_batch_val, x_val, y_val, epochs),\n",
    "        validation_steps = n_batch_val,\n",
    "        epochs = epochs,\n",
    "        verbose = verbose,\n",
    "        callbacks = callbacks\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "182/182 [==============================] - 43s 235ms/step - loss: 0.1978 - Accuracy: 0.9242 - val_loss: 0.1827 - val_Accuracy: 0.9250\n",
      "Epoch 2/10\n",
      "182/182 [==============================] - 39s 216ms/step - loss: 0.1924 - Accuracy: 0.9241 - val_loss: 0.1786 - val_Accuracy: 0.9252\n",
      "Epoch 3/10\n",
      "182/182 [==============================] - 38s 209ms/step - loss: 0.1818 - Accuracy: 0.9262 - val_loss: 0.1725 - val_Accuracy: 0.9280\n",
      "Epoch 4/10\n",
      "182/182 [==============================] - 37s 201ms/step - loss: 0.1784 - Accuracy: 0.9268 - val_loss: 0.1664 - val_Accuracy: 0.9291\n",
      "Epoch 5/10\n",
      "182/182 [==============================] - 39s 213ms/step - loss: 0.1732 - Accuracy: 0.9275 - val_loss: 0.1735 - val_Accuracy: 0.9244\n",
      "Epoch 6/10\n",
      "182/182 [==============================] - 38s 208ms/step - loss: 0.1726 - Accuracy: 0.9277 - val_loss: 0.1617 - val_Accuracy: 0.9286\n",
      "Epoch 7/10\n",
      "182/182 [==============================] - 40s 217ms/step - loss: 0.1691 - Accuracy: 0.9280 - val_loss: 0.1577 - val_Accuracy: 0.9310\n",
      "Epoch 8/10\n",
      "182/182 [==============================] - 39s 213ms/step - loss: 0.1663 - Accuracy: 0.9291 - val_loss: 0.1657 - val_Accuracy: 0.9295\n",
      "Epoch 9/10\n",
      "182/182 [==============================] - 38s 207ms/step - loss: 0.1625 - Accuracy: 0.9300 - val_loss: 0.1587 - val_Accuracy: 0.9294\n"
     ]
    }
   ],
   "source": [
    "history = train_model(train_tokens, train_tags, val_tokens, val_tags, batch_size = 32, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: When evaluating results for each epoch, be careful to not use custom metrics (F1/F2 score) directly through keras metric function that is calculated during training(fit) as it is done on batches (not together) and might show misleading results. Keras inbuilt metrics (tensorflow 2) use specialised built-in accumulators, and the computations are made properly. However, note it is possible to use custom metrics by other methods like backend utilities, callbacks, base layer API's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(figsize = (8, 8))\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We observe our training loss is still decreasing at 10 epochs and validation loss is fluctating and might decrease further. We can increase the number of epochs to check the future trend of our losses and our model might still be underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b521233766fbc8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b521233766fbc8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### From Tensorboard, we can monitor and evaluate various options, like scalars, graphs, distributions, histogram, embeddings,profiler etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### We will be using CoNLL-2003 shared task as our evaluation metric. It measures the performance of the systems in terms of precision, recall and f1-score, where: \n",
    "- precision is the percentage of named entities found by the learning system that are correct. \n",
    "- Recall is the percentage of named entities present in the corpus that are found by the system. \n",
    "- A named entity is correct only if it is an exact match of the corresponding entity in the data file.\n",
    "#### We consider 3 scenarios here, namely :-\n",
    "- Surface string and entity type match\n",
    "- System hypothesized an entity\n",
    "- System misses an entity\n",
    "\n",
    "#### Note: We can use other evaluation metric for NER like Automatic Content Extraction (ACE) or Message Understanding Conference (MUC) which considers other scenarios for partial matches as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function which will calculate our evaluation metrics as per CoNLL-2003 evaluation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def _update_chunk(candidate, prev, current_tag, current_chunk, current_pos, prediction=False):\n",
    "    if candidate == 'B-' + current_tag:\n",
    "        if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
    "                current_chunk[-1].append(current_pos - 1)\n",
    "        current_chunk.append([current_pos])\n",
    "    elif candidate == 'I-' + current_tag:\n",
    "        if prediction and (current_pos == 0 or current_pos > 0 and prev.split('-', 1)[-1] != current_tag):\n",
    "            current_chunk.append([current_pos])\n",
    "        if not prediction and (current_pos == 0 or current_pos > 0 and prev == 'O'):\n",
    "            current_chunk.append([current_pos])\n",
    "    elif current_pos > 0 and prev.split('-', 1)[-1] == current_tag:\n",
    "        if len(current_chunk) > 0:\n",
    "            current_chunk[-1].append(current_pos - 1)\n",
    "\n",
    "def _update_last_chunk(current_chunk, current_pos):\n",
    "    if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
    "        current_chunk[-1].append(current_pos - 1)\n",
    "\n",
    "def _tag_precision_recall_f1(tp, fp, fn):\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp) * 100\n",
    "    if tp + fn > 0:\n",
    "        recall = tp / (tp + fn) * 100\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def _aggregate_metrics(results, total_correct):\n",
    "    total_true_entities = 0\n",
    "    total_predicted_entities = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    for tag, tag_metrics in results.items():\n",
    "        n_pred = tag_metrics['n_predicted_entities']\n",
    "        n_true = tag_metrics['n_true_entities']\n",
    "        total_true_entities += n_true\n",
    "        total_predicted_entities += n_pred\n",
    "        total_precision += tag_metrics['precision'] * n_pred\n",
    "        total_recall += tag_metrics['recall'] * n_true\n",
    "    \n",
    "    accuracy = 0\n",
    "    if total_true_entities > 0:\n",
    "        accuracy = total_correct / total_true_entities * 100\n",
    "    else:\n",
    "        print('CAUTION! Accuracy equals zero because there are no '\\\n",
    "              'correct entities. Check the correctness of your data.')\n",
    "    if total_predicted_entities > 0:\n",
    "        total_precision = total_precision / total_predicted_entities\n",
    "    total_recall = total_recall / total_true_entities\n",
    "    if total_precision + total_recall > 0:\n",
    "        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n",
    "    return total_true_entities, total_predicted_entities, \\\n",
    "           total_precision, total_recall, total_f1, accuracy\n",
    "\n",
    "def _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct):\n",
    "    print('processed {len} tokens ' \\\n",
    "          'with {tot_true} phrases; ' \\\n",
    "          'found: {tot_pred} phrases; ' \\\n",
    "          'correct: {tot_cor}.\\n'.format(len=n_tokens,\n",
    "                                         tot_true=total_true_entities,\n",
    "                                         tot_pred=total_predicted_entities,\n",
    "                                         tot_cor=total_correct))\n",
    "\n",
    "def _print_metrics(accuracy, total_precision, total_recall, total_f1):\n",
    "    print('precision:  {tot_prec:.2f}%; ' \\\n",
    "          'recall:  {tot_recall:.2f}%; ' \\\n",
    "          'F1:  {tot_f1:.2f}%\\n'.format(acc=accuracy,\n",
    "                                           tot_prec=total_precision,\n",
    "                                           tot_recall=total_recall,\n",
    "                                           tot_f1=total_f1))\n",
    "\n",
    "def _print_tag_metrics(tag, tag_results):\n",
    "    print(('\\t%12s' % tag) + ': precision:  {tot_prec:6.2f}%; ' \\\n",
    "                               'recall:  {tot_recall:6.2f}%; ' \\\n",
    "                               'F1:  {tot_f1:6.2f}; ' \\\n",
    "                               'predicted:  {tot_predicted:4d}\\n'.format(tot_prec=tag_results['precision'],\n",
    "                                                                         tot_recall=tag_results['recall'],\n",
    "                                                                         tot_f1=tag_results['f1'],\n",
    "                                                                         tot_predicted=tag_results['n_predicted_entities']))\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred, print_results=True, short_report=False):\n",
    "    # Find all tags\n",
    "    tags = sorted(set(tag[2:] for tag in y_true + y_pred if tag != 'O'))\n",
    "\n",
    "    results = OrderedDict((tag, OrderedDict()) for tag in tags)\n",
    "    n_tokens = len(y_true)\n",
    "    total_correct = 0\n",
    "\n",
    "    # For eval_conll_try we find all chunks in the ground truth and prediction\n",
    "    # For each chunk we store starting and ending indices\n",
    "    for tag in tags:\n",
    "        true_chunk = list()\n",
    "        predicted_chunk = list()\n",
    "        for position in range(n_tokens):\n",
    "            _update_chunk(y_true[position], y_true[position - 1], tag, true_chunk, position)\n",
    "            _update_chunk(y_pred[position], y_pred[position - 1], tag, predicted_chunk, position, True)\n",
    "\n",
    "        _update_last_chunk(true_chunk, position)\n",
    "        _update_last_chunk(predicted_chunk, position)\n",
    "\n",
    "        # Then we find all correctly classified intervals\n",
    "        # True positive results\n",
    "        tp = sum(chunk in predicted_chunk for chunk in true_chunk)\n",
    "        total_correct += tp\n",
    "\n",
    "        # And then just calculate errors of the first and second kind\n",
    "        # False negative\n",
    "        fn = len(true_chunk) - tp\n",
    "        # False positive\n",
    "        fp = len(predicted_chunk) - tp\n",
    "        precision, recall, f1 = _tag_precision_recall_f1(tp, fp, fn)\n",
    "\n",
    "        results[tag]['precision'] = precision\n",
    "        results[tag]['recall'] = recall\n",
    "        results[tag]['f1'] = f1\n",
    "        results[tag]['n_predicted_entities'] = len(predicted_chunk)\n",
    "        results[tag]['n_true_entities'] = len(true_chunk)\n",
    "\n",
    "    total_true_entities, total_predicted_entities, \\\n",
    "           total_precision, total_recall, total_f1, accuracy = _aggregate_metrics(results, total_correct)\n",
    "\n",
    "    if print_results:\n",
    "        _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct)\n",
    "        _print_metrics(accuracy, total_precision, total_recall, total_f1)\n",
    "\n",
    "        if not short_report:\n",
    "            for tag, tag_results in results.items():\n",
    "                _print_tag_metrics(tag, tag_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(token_idxs_):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    token_idxs_batch = np.expand_dims(token_idxs_,0)\n",
    "    tag_idxs_batch = model.predict(token_idxs_batch)\n",
    "    tag_idxs_batch = np.argmax(tag_idxs_batch, axis = -1)\n",
    "\n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):            \n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)        \n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(tokens, tags, short_report = True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x_batch, y_batch in zip(tokens, tags):\n",
    "        x_batch_id = words2idxs(x_batch)\n",
    "        y_batch_id = tags2idxs(y_batch)\n",
    "        tags_batch, tokens_batch = predict_tags(x_batch_id)\n",
    "        y_true_batch = np.expand_dims(y_batch_id,0)      \n",
    "        \n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_true_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "          \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results = True, short_report = short_report)\n",
    "    return (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After defining  our evaluation metrics, we can finally evaualte our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 11742 tokens with 425 phrases; found: 123 phrases; correct: 56.\n",
      "\n",
      "precision:  45.53%; recall:  13.18%; F1:  20.44%\n",
      "\n",
      "\t     company: precision:   80.00%; recall:    7.02%; F1:   12.90; predicted:     5\n",
      "\n",
      "\t    facility: precision:   37.21%; recall:   39.02%; F1:   38.10; predicted:    43\n",
      "\n",
      "\t     geo-loc: precision:   64.71%; recall:   27.27%; F1:   38.37; predicted:    51\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t musicartist: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t       other: precision:    6.67%; recall:    1.32%; F1:    2.20; predicted:    15\n",
      "\n",
      "\t      person: precision:   22.22%; recall:    2.86%; F1:    5.06; predicted:     9\n",
      "\n",
      "\t     product: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t  sportsteam: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = eval_conll(test_tokens, test_tags, short_report = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe a F1 score of about 21%. We can conduct more experiments to tune our hyperparameters and compare our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For our small training corpora (in comparison with usual sizes of corpora in Deep Learning), our results are very poor in comparison to the previous methods. As mentioned earlier, our model might also be underfitting and we could increase the number of epochs to see if it improves our prediction. We could try pre-trained GloVe embedding in combination with Dense layer in output(previous models) or try using TimeDistributed layer without using Glove Embedding. We could also try using some other pre-trained embedding like 'Gensim's Wordtovec', ''Google's word2cec', 'Wikipedia2Vec', 'SpaCy' etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
