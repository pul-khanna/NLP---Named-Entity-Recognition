{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><b> Named Entity Recognition </b></center></h1>\n",
    "\n",
    "#### NER is used for extraction such entities from the text as persons, organizations, locations, etc. Here we try to recognize named entities from Twitter with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will use a recurrent neural network model which will be made using KERAS Sequentional API to solve our NER problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide tensorflow warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Activation, Dropout, Bidirectional\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Sometimes we have to run the above cell for importing libraries again as tensorflow or keras module is unable to get imported the first time due to some bug. If later on, if we get an error when compiling our mode or our settings optimizers etc, re-run the import library cell again.\n",
    "#### Note: Tensorflow 2.5 and numpy 1.20 and 1.21 will result in an error when building the model as Tensforflow uses numpy ver 1.19.5 internally for calculations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT O\\n', '@TheValarium O\\n', ': O\\n', 'Online O\\n', 'ticket O\\n', 'sales O\\n', 'for O\\n', 'Ghostland B-musicartist\\n', 'Observatory I-musicartist\\n', 'extended O\\n', 'until O\\n', '6 O\\n', 'PM O\\n', 'EST O\\n', 'due O\\n', 'to O\\n', 'high O\\n', 'demand O\\n', '. O\\n', 'Get O\\n', 'them O\\n', 'before O\\n', 'they O\\n', 'sell O\\n', 'out O\\n', '... O\\n', '\\n', 'Apple B-product\\n', 'MacBook I-product\\n', 'Pro I-product\\n', 'A1278 I-product\\n', '13.3 I-product\\n', '\" I-product\\n', 'Laptop I-product\\n', '- I-product\\n', 'MD101LL/A I-product\\n', '( O\\n', 'June O\\n', ', O\\n', '2012 O\\n', ') O\\n', '- O\\n', 'Full O\\n', 'read O\\n', 'by O\\n', 'eBay B-company\\n', 'http://t.co/2zgQ99nmuf O\\n', 'http://t.co/eQmogqqABK O\\n', '\\n', 'Happy O\\n', 'Birthday O\\n', '@AshForeverAshey O\\n', '! O\\n', 'May O\\n', 'Allah B-person\\n', 's.w.t O\\n', 'bless O\\n', 'you O\\n', 'with O\\n', 'goodness O\\n', 'and O\\n', 'happiness O\\n', '. O\\n', '\\n', '@AqwSkills O\\n', 'the O\\n', 'quest O\\n', 'line O\\n', 'im O\\n', 'assuming O\\n', 'it O\\n', 'will O\\n', 'be O\\n', 'the O\\n', 'same O\\n', 'way O\\n', 'with O\\n', 'awe O\\n', 'thur O\\n', '\\n', '@Wolven O\\n', '@VanessaY O\\n', '@miniver O\\n', '@mathpunk O\\n', '@NeilHarbisson O\\n', 'still O\\n', 'perception O\\n', ', O\\n', 'and O\\n', 'what O\\n', 'you O\\n', 'key O\\n', 'off O\\n', 'may O\\n', 'differ O\\n', 'from O\\n', 'me O\\n', '. O\\n', 'Back O\\n', 'to O\\n']\n"
     ]
    }
   ],
   "source": [
    "# Note: 'with' automatically closes file \n",
    "\n",
    "with open('data/train.txt') as f:\n",
    "    line = [f.readline() for i in range(100)]\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading twitter text file format, we note every line contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace and different tweets are separated by an empty line. \n",
    "\n",
    "#### Further, we see different user names start after an '@' symbol and also there are different url's. These can be replaced by an identification token as they won't be useful by themselves in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to read twitter text file and return a separated list of tokens and its corresponding tags \"\"\"\n",
    "\n",
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "\n",
    "            if token.startswith(\"@\"):\n",
    "                token=\"<USR>\"\n",
    "            elif token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
    "                token=\"<URL>\"\n",
    "            \n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag)\n",
    "            \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "val_tokens, val_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving our tokens and tags into train,validation and test sets.\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Sentence 1 :- -----\n",
      "RT\tO\n",
      "<USR>\tO\n",
      ":\tO\n",
      "Online\tO\n",
      "ticket\tO\n",
      "sales\tO\n",
      "for\tO\n",
      "Ghostland\tB-musicartist\n",
      "Observatory\tI-musicartist\n",
      "extended\tO\n",
      "until\tO\n",
      "6\tO\n",
      "PM\tO\n",
      "EST\tO\n",
      "due\tO\n",
      "to\tO\n",
      "high\tO\n",
      "demand\tO\n",
      ".\tO\n",
      "Get\tO\n",
      "them\tO\n",
      "before\tO\n",
      "they\tO\n",
      "sell\tO\n",
      "out\tO\n",
      "...\tO\n",
      "----------------------- \n",
      "\n",
      "----- Sentence 2 :- -----\n",
      "Apple\tB-product\n",
      "MacBook\tI-product\n",
      "Pro\tI-product\n",
      "A1278\tI-product\n",
      "13.3\tI-product\n",
      "\"\tI-product\n",
      "Laptop\tI-product\n",
      "-\tI-product\n",
      "MD101LL/A\tI-product\n",
      "(\tO\n",
      "June\tO\n",
      ",\tO\n",
      "2012\tO\n",
      ")\tO\n",
      "-\tO\n",
      "Full\tO\n",
      "read\tO\n",
      "by\tO\n",
      "eBay\tB-company\n",
      "<URL>\tO\n",
      "<URL>\tO\n",
      "----------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"----- Sentence {0} :- -----\".format(i+1))\n",
    "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
    "        print('%s\\t%s' % (token, tag))\n",
    "    print (\"----------------------- \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking our training set list for tokens and tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To train our neural network, we will create a dictionary with two mappings:\n",
    "- token → token id: use integers as input data of KERAS embedding layer so that words can be represented     by dense vectors (More memory and computationally efficient than One-Hot Encoding)\n",
    "- tag → tag id: use integers for computing the loss at the output of the network.\n",
    "\n",
    "#### Note: Here we are considering that we have single-class labels, where an object can only belong to one class, i.e. each token belongs to a particular tag or vice-versa. Hence, we can use \"sparse_categorical_crossentropy\" instead of \"categorical_crossentropy\" as our loss function which requires one-hot-encoded vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: We will also need to add some 'Special tokens and tags' to our dictionary. They will be unknown tokens and padding tokens/tags.\n",
    "- &lt;UNK&gt; token to represent of vocabulary tokens;\n",
    "- &lt;PAD&gt; tokens and tags for padding sentence to the same length when we create batches of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dictionary to create mappings for tokens and tags \"\"\"\n",
    "\n",
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    \"\"\"\n",
    "        tokens_or_tags: a list of lists of tokens or tags\n",
    "        special_tokens: some special tokens\n",
    "    \"\"\"\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "# Note: First add special tokens and tags to the dictionaries and the first special token must have index 0\n",
    "# Here, first special token must be for \"pad\" as later on we can use an in-built KERAS function for masking\n",
    "# pads to avoid extra copmutations i.e. mask_zero which only applies for value 0 (index 0 here)\n",
    "    \n",
    "    k = 0\n",
    "    for line in special_tokens:\n",
    "        tok2idx[line] = k\n",
    "        k += 1\n",
    "        idx2tok.append(line)\n",
    "        \n",
    "    for tokens in tokens_or_tags:\n",
    "        for token in tokens:\n",
    "            if token not in tok2idx: \n",
    "                tok2idx[token] = k \n",
    "                k += 1\n",
    "                idx2tok.append(token)\n",
    "    \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<PAD>', '<UNK>']\n",
    "special_tags = ['<PAD>']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens + val_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags + val_tags , special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Implemented the function build_dict and creating dictionaries for our tokens and tags.\n",
    "#### Note: We always consider test tokens and tags to be unseen/unknown and hence don't use them when creating our dicitonaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Input Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The LSTM layers only accepts sequences within a batch that are of same lengths. Here, we will work with the max length of the longest sentence (sequence) and pad the shorter sentences (sequences) to achieve this. Thus all batches of input data will be of the same length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: This will not be as efficient in comparison to bucketing technique(varying input lengths) but for sake for simplicity we will use this. \n",
    "\n",
    "#### To use bucketing technique we will have to set our 'input_length' params in 'Embedding' layer to 'None'. However, it will fail in a scenario where we decide to flatten and dense our layers later as sequentional API requires constant input_length when the param is set to 'None' (In this scenario we can use Functional API). Also, note this problem is more common in ConvNets as they are multidimensional(except Conv1d) and Dense layers require 1-D data at all times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(s) for s in (train_tokens + val_tokens)])\n",
    "\n",
    "\"\"\" Function for adding pads to all sentences so as to make all of them of equal length \"\"\"\n",
    "\n",
    "def pad_data(data, mapper, maxlen = maxlen, padding = \"post\"):\n",
    "    \n",
    "    \"\"\" 'data' refers to the dataset which we want to pad. e.g train_tokens\n",
    "    'mapper' refers to string input whether dataset is tokens or tags. e.g \"tokens\"\n",
    "    'maxlen' refers to max length of the sentence from batch that we want to set as references for padding\n",
    "    'padding' refers to the position where we want to pad.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if mapper == 'tokens':\n",
    "        seq_list = [[token2idx[w] for w in s] for s in data]\n",
    "        pad = pad_sequences(maxlen = maxlen, sequences = seq_list, padding = \"post\", value = token2idx[\"<PAD>\"])\n",
    "    else:\n",
    "        seq_list = [[tag2idx[w] for w in s] for s in data]\n",
    "        pad = pad_sequences(maxlen = maxlen, sequences = seq_list, padding = \"post\", value = tag2idx[\"<PAD>\"])\n",
    "        \n",
    "    return pad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_data(train_tokens, mapper = 'tokens')\n",
    "X_val = pad_data(val_tokens, mapper = 'tokens')\n",
    "X_test = pad_data(test_tokens, mapper = 'tokens')\n",
    "\n",
    "y_train = pad_data(train_tags, mapper = 'tags')\n",
    "y_val = pad_data(val_tags, mapper = 'tags')\n",
    "y_test = pad_data(test_tags, mapper = 'tags')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have set all our tokens and corresponding tags sentences of equal length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Model Architecture\n",
    "\n",
    "#### We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification. We will use Keras Sequentional API to build our model.\n",
    "\n",
    "#### 'mask_zero' param will be set to True in 'Embedding' layer so as to to avoid computations for padding tokens inside the RNN and also when computing loss function.\n",
    "\n",
    "#### We will set 'dropout' param in our RNN layer for regularization.\n",
    "\n",
    "#### We will use 'softmax' in our activation function for output lyer, as it will provide us a multinominal probability distribution for our multi-label classification problem\n",
    "\n",
    "#### Our Loss Function will be \"sparse_categorical_crossentropy\" (No need for OHC, can use integers)\n",
    "\n",
    "#### Note: We can use 'Time Distributed Layer'(Many to Many architecture) instead of 'Dense Layer'(Many to One architecture) as output layer and check its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "- input_dim: This is the size of the vocabulary in the text data. Here, it will length of token dictionary   consisting of \n",
    "- output_dim: This is the size of the vector space in which words will be embedded. It defines the size of   the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger.       Here, we will use 200.\n",
    "- input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. Here, it will max sentence length from our training and validations dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The output dense layer length should be equal to number of labels i.e. tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(idx2token)\n",
    "output_dim = 200\n",
    "input_length = maxlen\n",
    "num_tags = len(idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialized input and output parameters for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Optimizer for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use adam optimization algorithm with learning rate decay. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. An initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.\n",
    "#### Note: We can also use 'Adaptive Moment Estimation' instead of learning rate schedules method as it can be challenging to configure and is critical to the performance of a deep learning neural network model. \n",
    "\n",
    "#### We also apply clipping to eliminate exploding gradients. Here, we use 'global_clipnorm' function in KERAS which clips only gradients and not variables. Also, function 'clip_norm' clips the gradient of each weight independently of the gradients of the other weights and thus has the disadvantage of changing the descent direction, whereas with global_clipnorm the direction would remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate = 1e-2,\n",
    "    decay_steps = 10000,\n",
    "    decay_rate = 0.9)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate = lr_schedule, global_clipnorm = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS BI-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 41, 200)           4101000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 41, 200)           240800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 41, 22)            4422      \n",
      "=================================================================\n",
      "Total params: 4,346,222\n",
      "Trainable params: 4,346,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length, mask_zero = True))\n",
    "\n",
    "forward_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(100, dropout = 0.5), return_sequences = True)\n",
    "backward_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(100, dropout = 0.5), return_sequences = True, go_backwards = True)\n",
    "\n",
    "model.add(Bidirectional(forward_layer, backward_layer = backward_layer))\n",
    "\n",
    "model.add(Dense(num_tags, activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"Accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After compiling, we will train our data and conduct more experiments as needed to tune our hyperparameters, to obtain higher accuracy on the held-out validation dataset.\n",
    "#### The hyperparameters for our model are :- batch_size, epochs, learning_rate params, global_clip_norm, dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "182/182 [==============================] - 68s 371ms/step - loss: 0.1661 - Accuracy: 0.9278 - val_loss: 0.1136 - val_Accuracy: 0.9372\n",
      "Epoch 2/5\n",
      "182/182 [==============================] - 59s 324ms/step - loss: 0.0724 - Accuracy: 0.9547 - val_loss: 0.1072 - val_Accuracy: 0.9395\n",
      "Epoch 3/5\n",
      "182/182 [==============================] - 53s 292ms/step - loss: 0.0336 - Accuracy: 0.9778 - val_loss: 0.1203 - val_Accuracy: 0.9364\n",
      "Epoch 4/5\n",
      "182/182 [==============================] - 54s 299ms/step - loss: 0.0178 - Accuracy: 0.9874 - val_loss: 0.1262 - val_Accuracy: 0.9362\n",
      "Epoch 5/5\n",
      "182/182 [==============================] - 59s 326ms/step - loss: 0.0109 - Accuracy: 0.9918 - val_loss: 0.1381 - val_Accuracy: 0.9306\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    validation_data = (X_val , y_val),\n",
    "    batch_size = 32, \n",
    "    epochs = 5,\n",
    "    verbose = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: When evaluating results for each epoch, be careful to not use custom metrics (F1/F2 score) directly through keras metric function that is calculated during training(fit) as it is done on batches (not together) and might show misleading results. Keras inbuilt metrics (tensorflow 2) use specialised built-in accumulators, and the computations are made properly. However, note it is possible to use custom metrics by other methods like backend utilities, callbacks, base layer API's etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will plot the graph between the loss and number of epochs for training and validation set. To assess the number of epochs required and to avoid underfitting or overfitting, it is important to compare training and validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(figsize = (8, 8))\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe our training loss starts becoming constant around 5 epochs and validation loss is slightly increasing. We can increase the number of epochs to check the future trend of our losses.\n",
    "\n",
    "#### Note: We can use Callbacks to get a view on internal states and statistics of the model during training and thus help us in preventing overfitting, visualize training progress, debug your code, save checkpoints, generate logs etc. We have not used here for sake of simplicity and will demonstrate in later parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be using CoNLL-2003 shared task as our evaluation metric. It measures the performance of the systems in terms of precision, recall and f1-score, where: \n",
    "- precision is the percentage of named entities found by the learning system that are correct. \n",
    "- Recall is the percentage of named entities present in the corpus that are found by the system. \n",
    "- A named entity is correct only if it is an exact match of the corresponding entity in the data file.\n",
    "#### We consider 3 scenarios here, namely :-\n",
    "- Surface string and entity type match\n",
    "- System hypothesized an entity\n",
    "- System misses an entity\n",
    "\n",
    "#### Note: We can use other evaluation metric for NER like Automatic Content Extraction (ACE) or Message Understanding Conference (MUC) which considers other scenarios for partial matches as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function which will calculate our evaluation metrics as per CoNLL-2003 evaluation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def _update_chunk(candidate, prev, current_tag, current_chunk, current_pos, prediction=False):\n",
    "    if candidate == 'B-' + current_tag:\n",
    "        if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
    "                current_chunk[-1].append(current_pos - 1)\n",
    "        current_chunk.append([current_pos])\n",
    "    elif candidate == 'I-' + current_tag:\n",
    "        if prediction and (current_pos == 0 or current_pos > 0 and prev.split('-', 1)[-1] != current_tag):\n",
    "            current_chunk.append([current_pos])\n",
    "        if not prediction and (current_pos == 0 or current_pos > 0 and prev == 'O'):\n",
    "            current_chunk.append([current_pos])\n",
    "    elif current_pos > 0 and prev.split('-', 1)[-1] == current_tag:\n",
    "        if len(current_chunk) > 0:\n",
    "            current_chunk[-1].append(current_pos - 1)\n",
    "\n",
    "def _update_last_chunk(current_chunk, current_pos):\n",
    "    if len(current_chunk) > 0 and len(current_chunk[-1]) == 1:\n",
    "        current_chunk[-1].append(current_pos - 1)\n",
    "\n",
    "def _tag_precision_recall_f1(tp, fp, fn):\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    if tp + fp > 0:\n",
    "        precision = tp / (tp + fp) * 100\n",
    "    if tp + fn > 0:\n",
    "        recall = tp / (tp + fn) * 100\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def _aggregate_metrics(results, total_correct):\n",
    "    total_true_entities = 0\n",
    "    total_predicted_entities = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    for tag, tag_metrics in results.items():\n",
    "        n_pred = tag_metrics['n_predicted_entities']\n",
    "        n_true = tag_metrics['n_true_entities']\n",
    "        total_true_entities += n_true\n",
    "        total_predicted_entities += n_pred\n",
    "        total_precision += tag_metrics['precision'] * n_pred\n",
    "        total_recall += tag_metrics['recall'] * n_true\n",
    "    \n",
    "    accuracy = 0\n",
    "    if total_true_entities > 0:\n",
    "        accuracy = total_correct / total_true_entities * 100\n",
    "    else:\n",
    "        print('CAUTION! Accuracy equals zero because there are no '\\\n",
    "              'correct entities. Check the correctness of your data.')\n",
    "    if total_predicted_entities > 0:\n",
    "        total_precision = total_precision / total_predicted_entities\n",
    "    total_recall = total_recall / total_true_entities\n",
    "    if total_precision + total_recall > 0:\n",
    "        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n",
    "    return total_true_entities, total_predicted_entities, \\\n",
    "           total_precision, total_recall, total_f1, accuracy\n",
    "\n",
    "def _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct):\n",
    "    print('processed {len} tokens ' \\\n",
    "          'with {tot_true} phrases; ' \\\n",
    "          'found: {tot_pred} phrases; ' \\\n",
    "          'correct: {tot_cor}.\\n'.format(len=n_tokens,\n",
    "                                         tot_true=total_true_entities,\n",
    "                                         tot_pred=total_predicted_entities,\n",
    "                                         tot_cor=total_correct))\n",
    "\n",
    "def _print_metrics(accuracy, total_precision, total_recall, total_f1):\n",
    "    print('precision:  {tot_prec:.2f}%; ' \\\n",
    "          'recall:  {tot_recall:.2f}%; ' \\\n",
    "          'F1:  {tot_f1:.2f}%\\n'.format(acc=accuracy,\n",
    "                                           tot_prec=total_precision,\n",
    "                                           tot_recall=total_recall,\n",
    "                                           tot_f1=total_f1))\n",
    "\n",
    "def _print_tag_metrics(tag, tag_results):\n",
    "    print(('\\t%12s' % tag) + ': precision:  {tot_prec:6.2f}%; ' \\\n",
    "                               'recall:  {tot_recall:6.2f}%; ' \\\n",
    "                               'F1:  {tot_f1:6.2f}; ' \\\n",
    "                               'predicted:  {tot_predicted:4d}\\n'.format(tot_prec=tag_results['precision'],\n",
    "                                                                         tot_recall=tag_results['recall'],\n",
    "                                                                         tot_f1=tag_results['f1'],\n",
    "                                                                         tot_predicted=tag_results['n_predicted_entities']))\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred, print_results=True, short_report=False):\n",
    "    # Find all tags\n",
    "    tags = sorted(set(tag[2:] for tag in y_true + y_pred if tag != 'O'))\n",
    "\n",
    "    results = OrderedDict((tag, OrderedDict()) for tag in tags)\n",
    "    n_tokens = len(y_true)\n",
    "    total_correct = 0\n",
    "\n",
    "    # For eval_conll_try we find all chunks in the ground truth and prediction\n",
    "    # For each chunk we store starting and ending indices\n",
    "    for tag in tags:\n",
    "        true_chunk = list()\n",
    "        predicted_chunk = list()\n",
    "        for position in range(n_tokens):\n",
    "            _update_chunk(y_true[position], y_true[position - 1], tag, true_chunk, position)\n",
    "            _update_chunk(y_pred[position], y_pred[position - 1], tag, predicted_chunk, position, True)\n",
    "\n",
    "        _update_last_chunk(true_chunk, position)\n",
    "        _update_last_chunk(predicted_chunk, position)\n",
    "\n",
    "        # Then we find all correctly classified intervals\n",
    "        # True positive results\n",
    "        tp = sum(chunk in predicted_chunk for chunk in true_chunk)\n",
    "        total_correct += tp\n",
    "\n",
    "        # And then just calculate errors of the first and second kind\n",
    "        # False negative\n",
    "        fn = len(true_chunk) - tp\n",
    "        # False positive\n",
    "        fp = len(predicted_chunk) - tp\n",
    "        precision, recall, f1 = _tag_precision_recall_f1(tp, fp, fn)\n",
    "\n",
    "        results[tag]['precision'] = precision\n",
    "        results[tag]['recall'] = recall\n",
    "        results[tag]['f1'] = f1\n",
    "        results[tag]['n_predicted_entities'] = len(predicted_chunk)\n",
    "        results[tag]['n_true_entities'] = len(true_chunk)\n",
    "\n",
    "    total_true_entities, total_predicted_entities, \\\n",
    "           total_precision, total_recall, total_f1, accuracy = _aggregate_metrics(results, total_correct)\n",
    "\n",
    "    if print_results:\n",
    "        _print_info(n_tokens, total_true_entities, total_predicted_entities, total_correct)\n",
    "        _print_metrics(accuracy, total_precision, total_recall, total_f1)\n",
    "\n",
    "        if not short_report:\n",
    "            for tag, tag_results in results.items():\n",
    "                _print_tag_metrics(tag, tag_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(token_idxs_):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    token_idxs_batch = np.expand_dims(token_idxs_,0)\n",
    "    tag_idxs_batch = model.predict(token_idxs_batch)\n",
    "    tag_idxs_batch = np.argmax(tag_idxs_batch, axis = -1)\n",
    "\n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            \n",
    "            tags.append(idx2tag[tag_idx])\n",
    "            tokens.append(idx2token[token_idx])\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "        \n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(tokens, tags, short_report = True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch in zip(tokens, tags):        \n",
    "        tags_batch, tokens_batch = predict_tags(x_batch)\n",
    "        y_true_batch = np.expand_dims(y_batch,0)\n",
    "      \n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_true_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        # We extend every prediction and ground truth sequence with 'O' tag\n",
    "        # to indicate a possible end of entity.\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "          \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report = short_report)\n",
    "    return (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After defining  our evaluation metrics, we can finally evaualte our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 11742 tokens with 425 phrases; found: 456 phrases; correct: 255.\n",
      "\n",
      "precision:  55.92%; recall:  60.00%; F1:  57.89%\n",
      "\n",
      "\t     company: precision:   62.12%; recall:   71.93%; F1:   66.67; predicted:    66\n",
      "\n",
      "\t    facility: precision:   48.65%; recall:   43.90%; F1:   46.15; predicted:    37\n",
      "\n",
      "\t     geo-loc: precision:   73.77%; recall:   74.38%; F1:   74.07; predicted:   122\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     5\n",
      "\n",
      "\t musicartist: precision:   23.53%; recall:   25.00%; F1:   24.24; predicted:    17\n",
      "\n",
      "\t       other: precision:   41.41%; recall:   53.95%; F1:   46.86; predicted:    99\n",
      "\n",
      "\t      person: precision:   77.05%; recall:   67.14%; F1:   71.76; predicted:    61\n",
      "\n",
      "\t     product: precision:   29.41%; recall:   31.25%; F1:   30.30; predicted:    17\n",
      "\n",
      "\t  sportsteam: precision:   42.86%; recall:   47.37%; F1:   45.00; predicted:    21\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:    11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = eval_conll(X_test, y_test, short_report = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We observe a F1 score of about 58%. We can conduct more experiments to tune our hyperparameters and compare our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despite the fact that we used small training corpora (in comparison with usual sizes of corpora in Deep Learning), our results are quite good. In addition, in this task there are many possible named entities and for some of them we have only several dozens of trainig examples, which is definitely small. However, the implemented model outperforms classical CRFs for this task. Even better results could be obtained by some combinations of several types of methods and other techniques or model architectures and by carefully selecting the values of hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
